{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ```FFANN```\n",
    "\n",
    "Let's augment the ```FFANN``` class in order to be able to train it.\n",
    "\n",
    "This is still a work in progress, but at least it can \"fit\" the *XOR*-gate and a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeedForwardANN as FFANN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFv2(FFANN.FFANN):\n",
    "    '''\n",
    "    We will try to augment the FFANN class, in order to be able to train it.\n",
    "    Basically, we will add an optimizer. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, _inputs,_outputs,_hidden_nodes, activations, loss, strategy):\n",
    "        '''\n",
    "        Everything the same as the base class.\n",
    "        loss is the loss function.\n",
    "        strategy is our optimizer.\n",
    "        '''\n",
    "        super().__init__( _inputs,_outputs,_hidden_nodes, activations)\n",
    "        \n",
    "        #I don't like this. There has to be another, more elegant, way to do this...\n",
    "        self.loss=loss(self)\n",
    "        self.strategy=strategy(self)\n",
    "        \n",
    "    def SGD(self, data_in, data_out,alpha=1e-2, abs_tol=1e-5, rel_tol=1e-3, step_break=100,max_step=5000):\n",
    "        '''\n",
    "        This how I think the training (via stochastic gradient descent) should work.\n",
    "        You have to pass the data (data_in are the inputs and data_out their corresponding outputs),\n",
    "        and define the loss (and its derivative).\n",
    "        '''\n",
    "        #size of data\n",
    "        data_size=len(data_in)\n",
    "        \n",
    "        _s=0\n",
    "        count_steps=1\n",
    "        while count_steps<=max_step:\n",
    "            \n",
    "            #get a random data point (it will be passed to the strategy.update function)\n",
    "            index=np.random.randint(data_size)\n",
    "            x=data_in[index]\n",
    "            t=data_out[index]\n",
    "            #run for x as the input, in order to get the output signal and be prepeared\n",
    "            #to get its derivatives over the parameters.\n",
    "            self.inputSignal(x)\n",
    "            self.feedForward()\n",
    "            self.backPropagation()\n",
    "\n",
    "            \n",
    "            #strategy should have an update member. During this, w's and b's are updated.\n",
    "            #it can return sumething that becomes less than one, when the gradient becomes\n",
    "            #small enough (we can change it later, to make the convergence conditions more \n",
    "            #involved). \n",
    "            #It also should have access to \"self\" because it will need to know the \n",
    "            #structure of the network, the loss, and take their derivatives.\n",
    "            _check=self.strategy.update(t, alpha, abs_tol, rel_tol)\n",
    "            #print(self.loss(self.signals[self.total_layers-1],t) )\n",
    "            count_steps+=1             \n",
    "                \n",
    "            if _check<1:\n",
    "                _s+=1\n",
    "            else:\n",
    "                _s=0\n",
    "            \n",
    "            if _s>step_break:\n",
    "                break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss:\n",
    "    def __init__(self,FFANN):\n",
    "        self.nodes=FFANN.nodes\n",
    "        self.FFANN=FFANN\n",
    "        self.total_layers=self.FFANN.total_layers\n",
    "        self.N=self.nodes[self.total_layers-1]\n",
    "        \n",
    "        #I will use these to hold the derivatives wrt w and b from FFANN\n",
    "        self.dQdw=0\n",
    "        self.dQdb=0\n",
    "        \n",
    "    def __call__(self,signal_out,data_out):\n",
    "        sum_Q=0\n",
    "        \n",
    "        for r in range(self.N):\n",
    "            sum_Q+=(signal_out[r]-data_out[r])**2.\n",
    "        sum_Q=sum_Q/(float(self.N)) #take the average\n",
    "        return sum_Q\n",
    "    \n",
    "    def dQds(self, signal_out, data_out):\n",
    "        '''define a component of the derivative of Q (wrt signal_out)'''\n",
    "        \n",
    "        return 2*(signal_out - data_out)/(float(self.N))\n",
    "        \n",
    "    def grad(self,l,j,i,signal_out,data_out):\n",
    "        #calculate the derivatives wrt w^{(l)}_{ji} and b^{(l)}_{j}\n",
    "        \n",
    "        self.FFANN.derivative_bw(l,j,i)\n",
    "        #the derivative in general is \n",
    "        #\\dfrac{\\partial Q}{\\partial P} = \\dfrac{\\partial Q}{\\partial signal^{N-1}_{r}}\\dfrac{\\partial signal^{N-1}_{r}}{\\partial P}\n",
    "        self.dQdw=0\n",
    "        self.dQdb=0\n",
    "        \n",
    "        \n",
    "        for r in range(self.N):\n",
    "            self.dQdw += self.dQds(signal_out[r],data_out[r])*self.FFANN.dsdw[r]\n",
    "            self.dQdb += self.dQds(signal_out[r],data_out[r])*self.FFANN.dsdb[r]\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaSGD:\n",
    "    '''\n",
    "    Not the best (far from it) strategy, but the simplest. Will use it to test if the implementation works.\n",
    "    '''\n",
    "    def __init__(self,FFANN):\n",
    "        self.FFANN=FFANN\n",
    "    \n",
    "    def update(self, data_out,alpha=1e-2,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        during the update step, you calculate the gradient of Q\n",
    "        and update w and b. \n",
    "        '''\n",
    "        signal_out=self.FFANN.signals[self.FFANN.total_layers-1]\n",
    "        for l in range(self.FFANN.total_layers-1):\n",
    "            for j in range(self.FFANN.nodes[l+1]):\n",
    "                for i in range(self.FFANN.nodes[l]):\n",
    "                    self.FFANN.loss.grad(l,j,i,signal_out,data_out)\n",
    "                    \n",
    "                    self.FFANN.addToWeight(l,j,i, -alpha*self.FFANN.loss.dQdw)\n",
    "                \n",
    "                self.FFANN.addToBias(l,j, -alpha*self.FFANN.loss.dQdb)\n",
    "                \n",
    "                \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin=FFANN.linearActivation()\n",
    "sig=FFANN.sigmoidActivation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_in=[[_] for _ in np.linspace(0,1,25)]\n",
    "# data_out=[[2*_] for _ in np.linspace(0,1,25)]\n",
    "# brain=FFv2(1,1,[3,2,1],[lin,lin,lin,lin,lin],loss,VanillaSGD)\n",
    "# brain.init_params(-1,1)\n",
    "\n",
    "\n",
    "data_in=[[1,1],[0,0],[1,0],[0,1]]\n",
    "data_out=[[0],[0],[1],[1]]\n",
    "brain=FFv2(2,1,[3],[sig,lin],loss,VanillaSGD)\n",
    "brain.init_params(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.SGD(data_in, data_out,alpha=1e-1, abs_tol=1e-2, rel_tol=1e-3, step_break=5000,max_step=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point: [1, 1] \n",
      "target: [0] FFANN gives: [0.00015414293633497778] loss: 2.376004482196901e-08\n",
      "\n",
      "point: [0, 0] \n",
      "target: [0] FFANN gives: [8.05169216215873e-05] loss: 6.482974667416832e-09\n",
      "\n",
      "point: [1, 0] \n",
      "target: [1] FFANN gives: [0.9999937554773366] loss: 3.899406329373049e-11\n",
      "\n",
      "point: [0, 1] \n",
      "target: [1] FFANN gives: [1.0000084038923998] loss: 7.062540746666597e-11\n",
      "\n",
      "mean loss: 7.58815974003656e-09\n"
     ]
    }
   ],
   "source": [
    "Q=0\n",
    "for i,_ in enumerate(data_in):\n",
    "    print('point:',_,'\\n'\n",
    "        'target:',data_out[i],\n",
    "          'FFANN gives:',brain(_),\n",
    "          'loss:',brain.loss(brain(_),data_out[i]))\n",
    "    print('')\n",
    "    Q+=brain.loss(brain(_),data_out[i])\n",
    "    \n",
    "print('mean loss:', Q/float(len(data_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
