{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ```FFANN```\n",
    "\n",
    "Let's augment the ```FFANN``` class in order to be able to train it.\n",
    "\n",
    "This is still a work in progress, but at least it can \"fit\" the *XOR*-gate and a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FeedForwardANN as FFANN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSpropSGD:\n",
    "    '''\n",
    "    RMSpropSGD strategy. Better than vanilla, but still not that good\n",
    "    '''\n",
    "    def __init__(self,FFANN,loss,gamma=0.95,epsilon=1e-6,alpha=1e-2):\n",
    "        '''\n",
    "        FFANN: the feed-forward neural network\n",
    "        loss: the loss function\n",
    "        gamma: the decaying parameter\n",
    "        epsilon: safety parameter (to avoid division by 0)\n",
    "        alpha: learning rate\n",
    "\n",
    "        '''\n",
    "        self.FFANN=FFANN\n",
    "        self.loss=loss\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "\n",
    "        #counters for the decaying means         \n",
    "        self.meanWeights=[ [[0 for i in range(self.FFANN.nodes[l])] for j in range(self.FFANN.nodes[l+1])]  for l in range(self.FFANN.total_layers-1)]\n",
    "        self.meanBiases=[ [0 for j in range(self.FFANN.nodes[l+1])]  for l in range(self.FFANN.total_layers-1)]      \n",
    "\n",
    "        \n",
    "    def update(self, data_out,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        during the update step, you calculate the gradient of Q\n",
    "        and update w and b. \n",
    "        '''\n",
    "        #These are the output signals of FFANN. The update should run after\n",
    "        #FFANN.feedForward() and FFANN.backPropagation().\n",
    "        signal_out=self.FFANN.signals[self.FFANN.total_layers-1] \n",
    "            \n",
    "        #these will be used to determine if the stopping conditions are satisfied \n",
    "        _w2=0\n",
    "        _check=0\n",
    "\n",
    "        \n",
    "        for l in range(self.FFANN.total_layers-1):\n",
    "            for j in range(self.FFANN.nodes[l+1]):\n",
    "                for i in range(self.FFANN.nodes[l]):\n",
    "                    #get the grad of the loss. The results should be stored in loss.dQdw and loss.dQdb\n",
    "                    #This way it should be easy to update the weights and biases of FFANN\n",
    "                    self.loss.grad(l,j,i,signal_out,data_out)\n",
    "                    \n",
    "                    \n",
    "                    self.meanWeights[l][j][i]=self.gamma*self.meanWeights[l][j][i] + (1-self.gamma)*self.loss.dQdw**2 \n",
    "                    dw=self.alpha/np.sqrt( (self.meanWeights[l][j][i]+self.epsilon)  )*self.loss.dQdw\n",
    "                    \n",
    "                    #update the weight using loss.dQdw\n",
    "                    self.FFANN.addToWeight(l,j,i, -dw)\n",
    "\n",
    "                    _w2=abs_tol + self.FFANN.weights[l][j][i] * rel_tol\n",
    "                    _check+=(self.loss.dQdw/_w2)*(self.loss.dQdw/_w2)\n",
    "\n",
    "                #update the bias using loss.dQdb (it is the same for all i, so don't run loss.grad again).\n",
    "                self.meanBiases[l][j]=self.gamma*self.meanBiases[l][j] + (1-self.gamma)*self.loss.dQdb**2 \n",
    "                dw=self.alpha/np.sqrt( (self.meanBiases[l][j]+self.epsilon)  )*self.loss.dQdb\n",
    "                 \n",
    "                self.FFANN.addToBias(l,j, -dw)\n",
    "                \n",
    "                _w2=abs_tol + self.FFANN.biases[l][j] * rel_tol\n",
    "                _check+=(self.loss.dQdb/_w2)*(self.loss.dQdb/_w2)\n",
    "                \n",
    "                \n",
    "        _check=np.sqrt(1./self.loss.N *_check)\n",
    "        return _check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin=FFANN.linearActivation()\n",
    "sig=FFANN.sigmoidActivation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_in=[[_] for _ in np.linspace(0,1,250)]\n",
    "# data_out=[[2*_] for _ in np.linspace(0,1,250)]\n",
    "# brain=FFANN.FFANN(1,1,[3,2,1],[lin,lin,lin,lin,lin])\n",
    "# brain.init_params(-1,1)\n",
    "\n",
    "\n",
    "data_in=[[1,1],[0,0],[1,0],[0,1]]\n",
    "data_out=[[0],[0],[1],[1]]\n",
    "brain=FFANN.FFANN(2,1,[3],[sig,lin])\n",
    "brain.init_params(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_i(signal,target):\n",
    "    return (signal-target)**2.\n",
    "\n",
    "def dQds_i(signal,target):\n",
    "    return 2.*(signal-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=FFANN.loss(Q_i, dQds_i, brain)\n",
    "# strategy=FFANN.VanillaSGD(brain,Q,alpha=1e-1)\n",
    "strategy=FFANN.RMSpropSGD(brain,Q,gamma=0.995,epsilon=1e-5,alpha=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.SGD(strategy, data_in, data_out, abs_tol=1e-2, rel_tol=1e-3, step_break=150,max_step=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point: [1, 1] \n",
      "target: [0] FFANN gives: [-0.000158628125000837] loss: 2.516288204128117e-08\n",
      "\n",
      "point: [0, 0] \n",
      "target: [0] FFANN gives: [0.0004516103365411195] loss: 2.039518960707832e-07\n",
      "\n",
      "point: [1, 0] \n",
      "target: [1] FFANN gives: [0.9997477511742452] loss: 6.362947009467965e-08\n",
      "\n",
      "point: [0, 1] \n",
      "target: [1] FFANN gives: [0.9997928215967571] loss: 4.292289077028331e-08\n",
      "\n",
      "mean loss: 8.391678474425684e-08\n",
      "max loss: 2.039518960707832e-07\n"
     ]
    }
   ],
   "source": [
    "meanQ=0\n",
    "maxQ=0\n",
    "for i,_ in enumerate(data_in):\n",
    "    Qi=Q(brain(_),data_out[i])\n",
    "    print('point:',_,'\\n'\n",
    "       'target:',data_out[i],\n",
    "         'FFANN gives:',brain(_),\n",
    "         'loss:',Qi)\n",
    "    print('')\n",
    "    if Qi > maxQ:\n",
    "        maxQ=Qi\n",
    "    meanQ+=Q(brain(_),data_out[i])\n",
    "    \n",
    "print('mean loss:', meanQ/float(len(data_out)))\n",
    "print('max loss:', maxQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
