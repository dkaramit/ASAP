{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a class that defines a *feed-forwad artificial neural network*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such \"network\" is basically a (in principle) very complicated function, with (in principle) a lot of adjastable parameters that is used to approximate (or interpolate) functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is made up form neurons that are connected with each other. A feed-forwad artificial neural network takes inputs, transforms them by passing them through its \"hidden\" layer, and produces outputs.\n",
    "\n",
    "A feed-forwad artificial neural network looks loke the following figure:\n",
    "\n",
    "<img src=\"misc/FFANN.png\" style=\"height:250px\">\n",
    "\n",
    "where the triangles represent the action of the neuron (some simple function we define), the coloured circles represent the \"weights\", and the empty ones represent the \"biases\". The weights and biases are adjustable parameters that we tune, in order to get the outputs we need from given inputs.  We should note that neurons do not connect with all the other ones, but the entire network is separated in \"layers\", with each layer taking as input the output of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron takes a number of inputs coming from the previous layer. The neuron, then does the following:\n",
    "\n",
    "<img src=\"misc/Neuron.png\" style=\"height:450px\">\n",
    "\n",
    "That is, it takes the outputs from the previous layer ($l$), and the weights that represent the strength of the connection between the node $i$ of the $l$ layer and node $j$ of the $l+1$ layer (all these inputs and outputs are called \"signals\"). Then it adds all the different contributions and returns the output of which is:\n",
    "$$\n",
    "x^{(l)}_{j} = \\theta\\left( \\displaystyle \\sum_{i=0}^{n^{(l)}-1} w^{(l)}_{j i}  x^{(l)}_{i} + b^{(l+1)}_{j}\\right) \\; , \n",
    "$$\n",
    "\n",
    "with $\\theta$ the \"activation function\", which is some simple function we choose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFANN:\n",
    "    def __init__(self, _inputs,_outputs,_hidden_nodes, \n",
    "                 _hidden_func=lambda x:x, _hidden_func_deriv=lambda x:1,\n",
    "                 _output_func=lambda x:x, _output_func_deriv=lambda x:1):\n",
    "        '''\n",
    "        Constructor for the class.\n",
    "\n",
    "        _inputs: number of input nodes\n",
    "        _outputs: number of ouput nodes\n",
    "        \n",
    "        _hidden_nodes: list of number of layers in each hidden node\n",
    "        _hidden_func,_output_func: activation functions for hidden layers and output, respectively\n",
    "        _hidden_func_deriv,_output_func_deriv: derivatives of activation functions for hidden layers and output, respectively\n",
    "        '''\n",
    "        \n",
    "        self.inputs=_inputs\n",
    "        self.outputs=_outputs\n",
    "        self.layers=len(_hidden_nodes)\n",
    "        self.total_layers=2+self.layers #total layers = No. layers + input layer + output layer\n",
    "        \n",
    "        self.hidden_activation=_hidden_func\n",
    "        self.hidden_activation_deriv=_hidden_func_deriv\n",
    "        self.output_activation=_output_func\n",
    "        self.output_activation_deriv=_output_func_deriv\n",
    "\n",
    "        #an array to hold number of nodes at each layer\n",
    "        self.nodes=[_inputs]\n",
    "        \n",
    "        for _n in _hidden_nodes:\n",
    "            self.nodes.append(_n)\n",
    "            \n",
    "        self.nodes.append(_outputs)\n",
    "        \n",
    "            \n",
    "        \n",
    "        #declare the signals\n",
    "        self.signals=[ [0 for j in range(self.nodes[l])]  for l in range(self.total_layers)]\n",
    "        #in case you get confused, these are the indices\n",
    "        self.signal_indices=[ ['x^({0})_{1}'.format(l,j) for j in range(self.nodes[l])]  for l in range(self.total_layers)]\n",
    "        \n",
    "\n",
    "        \n",
    "        #declare the weights\n",
    "        self.weights=[ [[0 for i in range(self.nodes[l])] for j in range(self.nodes[l+1])]  for l in range(self.layers+1)]\n",
    "        #in case you get confused, these are the indices\n",
    "        self.weight_indices=[ [['w^({0})_{1}{2}'.format(l,j,i) for i in range(self.nodes[l])] for j in range(self.nodes[l+1])]  for l in range(self.layers+1)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #declare the biases\n",
    "        #By definition the biases of l=0 are zero, so we define biases[l][j] to refer to the l+1 layer.\n",
    "        #this will help as we will be able to put the biases in the same loop as the weights (see init_params for example).\n",
    "        self.biases=[ [0 for j in range(self.nodes[l+1])]  for l in range(self.layers+1)]        \n",
    "        #in case you get confused, these are the indices\n",
    "        self.bias_indices=[ ['b^({0})_{1}'.format(l+1,j) for j in range(self.nodes[l+1])]  for l in range(self.layers+1)]\n",
    "        \n",
    "    \n",
    "        self.init_params()#initialize the weights and biases\n",
    "                \n",
    "    #========================initializations========================#\n",
    "    def init_params(self):\n",
    "        '''\n",
    "        Function that initializes the weights and biasesby some random numbers.\n",
    "        '''\n",
    "        for l in range(self.layers+1):\n",
    "            for j in range(self.nodes[l+1]):\n",
    "                #You see, we defined biases in this way in order to manipulate them with the weights at the same loop. \n",
    "                self.biases[l][j] = (np.random.choice([-1,1])*np.random.random() )\n",
    "                for i in range(self.nodes[l]):\n",
    "                    self.weights[l][j][i] = (np.random.choice([-1,1])*np.random.random() )\n",
    "    \n",
    "    \n",
    "    def write_params(self):\n",
    "        '''\n",
    "            Returns biases and weights as a vector.\n",
    "            Can be used to pass the parameters to an optimization algorithm that takes vectors. \n",
    "        '''\n",
    "        X=[]\n",
    "        for l in range(self.layers+1):\n",
    "            for j in range(self.nodes[l+1]):\n",
    "                X.append(self.biases[l][j])\n",
    "                for i in range(self.nodes[l]):\n",
    "                    X.append(self.weights[l][j][i])\n",
    "        return X\n",
    "    \n",
    "    def read_params(self,X):\n",
    "        '''\n",
    "            Read and update biases and weights from a vector X\n",
    "            Can be used to read the result from an optimization algorithm that returns vectors. \n",
    "        '''\n",
    "        _s=0\n",
    "        for l in range(self.layers+1):\n",
    "            for j in range(self.nodes[l+1]):\n",
    "                self.update_bias(l,j,X[_s])\n",
    "                _s+=1\n",
    "                for i in range(self.nodes[l]):\n",
    "                    self.update_weight(l,j,i,X[_s])\n",
    "                    _s+=1\n",
    "    \n",
    "    def input_signal(self, x):\n",
    "        '''Read the input (x).'''\n",
    "        for j,_ in enumerate(x):\n",
    "            self.signals[0][j] = _\n",
    "    \n",
    "    def calc_signal(self, l,j):\n",
    "        '''calculate the output of the j node of layer l.'''\n",
    "        if l == 0:\n",
    "            print('for l=0 run self.input_signal')\n",
    "            return 0\n",
    "        \n",
    "        sum_wx = np.sum( [ self.weights[l-1][j][i] * xi for i,xi in enumerate(self.signals[l-1]) ] ) \n",
    "        \n",
    "        #Notice that self.biases[l-1][j] correspond to the bias of node j and layer l.\n",
    "        if l == self.total_layers-1:\n",
    "            self.signals[l][j] =  self.output_activation( sum_wx + self.biases[l-1][j]  )\n",
    "            return 0\n",
    "        \n",
    "        if l < self.total_layers-1:\n",
    "            self.signals[l][j] = self.hidden_activation( sum_wx + self.biases[l-1][j]  )\n",
    "            return 0    \n",
    "    \n",
    "    #========================Feed-Forward========================#\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        '''\n",
    "        Calculates the output of the network.\n",
    "        Also sets the signals (for later use)\n",
    "        '''\n",
    "        for l in range(1,self.total_layers):\n",
    "            for j in range(self.nodes[l]):\n",
    "                self.calc_signal(l,j)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    #========================update========================#\n",
    "\n",
    "    def update_weight(self,l,j,i,value):\n",
    "        '''\n",
    "        Change the value of w^{l}_{ji} to value\n",
    "        '''\n",
    "        self.weights[l][j][i]=value\n",
    "\n",
    "    def update_bias(self,l,j,value):\n",
    "        '''\n",
    "        Change the value of b^{l}_{j} to value\n",
    "        '''\n",
    "        self.biases[l][j]=value\n",
    "    #======================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = FFANN(2,1,[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1],\n",
       " [0.5333569489144613, 0.34008546591749245, 2.412298499002562],\n",
       " [0.060177639527672855]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is how you run it!\n",
    "brain.input_signal([2,1])\n",
    "brain.feed_forward()\n",
    "brain.signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that the read and write to vector do not mess-up the biases and matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = FFANN(2,1,[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[-0.20366425972085334, 0.818560173679066]],\n",
       "  [[0.34308224697805345], [-0.26406907419433423], [0.6200973023335733]],\n",
       "  [[0.2725832433397404, -0.7802710900794901, -0.8475099534208587]]],\n",
       " [[0.8983739866766525],\n",
       "  [-0.5023652620355369, 0.3863775479380248, -0.4436935960669719],\n",
       "  [-0.11896976767245182]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.weights,brain.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=brain.write_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.read_params(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[-0.20366425972085334, 0.818560173679066]],\n",
       "  [[0.34308224697805345], [-0.26406907419433423], [0.6200973023335733]],\n",
       "  [[0.2725832433397404, -0.7802710900794901, -0.8475099534208587]]],\n",
       " [[0.8983739866766525],\n",
       "  [-0.5023652620355369, 0.3863775479380248, -0.4436935960669719],\n",
       "  [-0.11896976767245182]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.weights,brain.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
