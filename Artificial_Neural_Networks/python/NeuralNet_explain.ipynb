{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a class that defines a *feed-forwad artificial neural network*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such \"network\" is basically a (in principle) very complicated function, with (in principle) a lot of adjastable parameters that is used to approximate (or interpolate) functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is made up form neurons that are connected with each other. A feed-forwad artificial neural network takes inputs, transforms them by passing them through its \"hidden\" layer, and produces outputs.\n",
    "\n",
    "A feed-forwad artificial neural network looks loke the following figure:\n",
    "\n",
    "<img src=\"misc/FFANN.png\" style=\"height:250px\">\n",
    "\n",
    "where the triangles represent the action of the neuron (some simple function we define), the coloured circles represent the \"weights\", and the empty ones represent the \"biases\". The weights and biases are adjustable parameters that we tune, in order to get the outputs we need from given inputs.  We should note that neurons do not connect with all the other ones, but the entire network is separated in \"layers\", with each layer taking as input the output of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neuron takes a number of inputs coming from the previous layer. The neuron, then does the following:\n",
    "\n",
    "<img src=\"misc/Neuron.png\" style=\"height:450px\">\n",
    "\n",
    "That is, it takes the outputs from the previous layer ($l$), and the weights that represent the strength of the connection between the node $i$ of the $l$ layer and node $j$ of the $l+1$ layer (all these inputs and outputs are called \"signals\"). Then it adds all the different contributions and returns the output of which is:\n",
    "$$\n",
    "s^{(l)}_{j} = \\theta\\left( \\displaystyle \\sum_{i=0}^{n^{(l)}-1} w^{(l)}_{j i}  s^{(l)}_{i} + b^{(l+1)}_{j}\\right) \\; , \n",
    "$$\n",
    "\n",
    "with $\\theta$ the \"activation function\", which is some simple function we choose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ```FFANN``` class\n",
    "\n",
    "the class ```FFANN``` defines a feed-forward artificial neural network with an arbitrary number of input/output nodes and hidden layers.\n",
    "\n",
    "\n",
    "## Functions and conventions\n",
    "\n",
    "\n",
    "The constructor is called using\n",
    "\n",
    "```FFANN( number_of_input_nodes, number_of_output_nodes, hidden_nodes, activation_functions)```\n",
    "\n",
    "```hidden_nodes``` a list that contains the number of nodes in each hidden layer \n",
    "(e.g. a list like [2,3,5] means that we have 3 hidden layers with 2,3,5 number of nodes respectively).\n",
    "\n",
    "The weights, biases, and signals are stored in ```FFANN.weights```, ```FFANN.biases```,```FFANN.signals```. For teh moment they can be initialized using ```FFANN.init_params```, but one can define their own initialization. It is **important** that the biases of the $l^{\\rm th}$ layer correspond to the ```FFANN.biases[l-1]```, because the input nodes do not have a bias.\n",
    "\n",
    "```activation_functions``` is a  list with activation functions in each node. Again (**important**) the activations of the $l^{\\rm th}$ layer correspond to the ```FFANN.activations[l-1]```, because there is one less activatioin needed (the output is just the output).\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The different ways to get the output of the network are:\n",
    "\n",
    "1. ```FFANN.evaluate(x)```, which calculates *only* the signals.\n",
    "\n",
    "1. ```FFANN(x)```, which calculates the signals and the \"local\" derivatives of the network with self.signals[0]=x. This stores all the signals in ```FFANN.signals``` and the local derivatives in ```self.derivatives[l][j][i]```$= \\dfrac{\\partial s^{l+1}_{j}}{\\partial s^{(l)}_{i}}$.\n",
    "\n",
    "1. ```FFANN.feedForwardDerivatives()```. Calculates the signals and the \"local\" derivatives of the network (at the current ```FFANN.signals[0]```) This feed forward function also fills ```self.totalDerivatives```, with ```self.totalDerivatives[l][j][i]``` $= \\dfrac{\\partial s^{l+1}_{j}}{\\partial s^{(0)}_{i}}$.\n",
    "\n",
    "After ```FFANN(x)``` one call ```FFANN.backPropagation```. This fills ```FFANN.Delta[f][j][i]``` $= \\dfrac{\\partial s^{[N-1)]}_{k}}{\\partial s^{[N-(f+2)]}_{i}}$, with $N$ the total numer of layers ($N-1$ is the output layer since we start counting form $l=0$), $f=0,1,2,...N-2$. *Notice that* ```Delta``` is like ```totalDerivatives``` but backwards (hence **backPropagation**).\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "As already mensioned there are two different ways to get the derivates.\n",
    "\n",
    "1. call ```FFANN.feedForwardDerivatives()```. After this you can find\n",
    "$\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(0)}_{i}}$ in ```FFANN.totalDerivatives[N-2][j][i]```.\n",
    "\n",
    "2. ```FFANN(x)``` and then ```FFANN.backPropagation()```. After this you can find\n",
    "$\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(0)}_{i}}$ in ```FFANN.Delta[N-2][j][i]```.\n",
    "\n",
    "\n",
    "Once you run ```FFANN.backPropagation()```, you can also calculate the derivatives with respect to the weights, biases, and signals. Simply run ```FFANN.derivative_bw(self,l,j,i)``` which stores $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}}$ and  $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial b^{(l+1)}_{j}}$ (for all $r=0,1,... \\# \\text{ output nodes}$) to ```FFANN.dsdw``` and ```FFANN.dsdb```, respectively.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Why two kinds of \"derivatives\"?\n",
    "\n",
    "Both use the chain rule to obtain derivatives. However:\n",
    "    \n",
    "1. ```FFANN.totalDerivatives``` is obtained by applying the chain-rule during feed forward \n",
    "(i.e. at the same time as the calculation of the signals!). Therefore, this is useful when the network is already trained, and we just need its derivatives (since they are calculated during the feed forward). \n",
    "\n",
    "2. ```FFANN.Delta``` is obtained by applying the chain-rule during back propagation\n",
    "(i.e. its elements are $\\dfrac{\\partial s^{[N-1)]}_{k}}{\\partial s^{[N-(f+2)]}_{i}}$ \n",
    "for $N= total \\ \\# \\ layers$, $f=N-(l+3)$ and $l<N-2$).\n",
    "This is useful as we train the network, as it cannot be caclulated during feed forward.\n",
    "\n",
    "\n",
    "The good thing is that both can be used to obtain the derivative of the outputs with respect to the signals. That is:\n",
    "$\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(0)}_{i}}$ = ```FFANN.Delta[N-2][j][i]``` = ```FFANN.totalDerivatives[N-2][j][i]```\n",
    "\n",
    "----\n",
    "\n",
    "## Numerical derivatives\n",
    "\n",
    "For testing, debugginf or any other purpose one can think of, the numerical derivatives can be obtained using \n",
    "```FFANN.totalNumericalDerivative()```, which stores $\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(0)}_{i}}$ in ```FFANN.numericalDerivatives[j][i]```. \n",
    "\n",
    "The numerical derivatives of $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}}$ and $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial b^{(l+1)}_{j}}$ (for all $r=0,1,... \\# \\text{ output nodes}$) are stored in ```FFANN.numerical_dsdw``` and ```FFANN.numerical_dsdb``` after  calling ```FFANN.numericalDerivative_bw(l,j,i)```.  It should be noted, that this function has to be called after (at least) ```FFANN.evaluate``` since it uses the fact that this derivative needs only to re-calculate the signals after the  $(l+1)^{(\\rm th)}$ layer. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives with respect to the inputs and weights\n",
    "\n",
    "\n",
    "During ```FFANN.feedForward``` we store all $\\dfrac{\\partial s^{(l+1)}_{j}}{\\partial s^{(l)}_{i}}= \\theta^{\\prime \\, (l+1)}_{j} w^{(l)}_{ji}$\n",
    "in ```FFANN.derivatives```. For a network with $N$ layers ($N-2$ hidden + $1$ input layer + $1$ output layer), we wish to calculate  \n",
    "\n",
    "* $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial s^{(0)}_{p}}$ . [#1](#1)\n",
    "* $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}} = \n",
    "\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial s^{(l+1)}_{j}} \\theta^{\\prime\\, (l+1)}_{j}s^{(l)}_{i}$ (no sum over $j$). [#2](#2)\n",
    "\n",
    "\n",
    "In order to do this we can accumulate\n",
    "$$\n",
    "\\Delta^{ (0) }_{j i} = \\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(N-2)}_{i}} \\\\\n",
    "\\Delta^{ (1) }_{j i} = \\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(N-3)}_{i}}=\n",
    "\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(N-2)}_{k_1}} \\cdot \n",
    "\\dfrac{\\partial s^{(N-2)}_{k_1}}{\\partial s^{(N-3)}_{i}}=\n",
    "\\Delta^{(0)}_{j k}\\cdot \\dfrac{\\partial s^{(N-2)}_{k}}{\\partial s^{(N-3)}_{i}}\n",
    "\\\\\n",
    "\\Delta^{ (2) }_{j i} = \\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(N-4)}_{i}}=\n",
    "\\dfrac{\\partial s^{(N-1)}_{j}}{\\partial s^{(N-2)}_{k_1}} \\cdot \n",
    "\\dfrac{\\partial s^{(N-2)}_{k_1}}{\\partial s^{(N-3)}_{k_2}}\\cdot \n",
    "\\dfrac{\\partial s^{(N-3)}_{k_2}}{\\partial s^{(N-4)}_{i}}=\n",
    "\\Delta^{(1)}_{j k}\\cdot  \\dfrac{\\partial s^{(N-3)}_{k}}{\\partial s^{(N-4)}_{i}}\\\\\n",
    "\\vdots\\\\\n",
    "\\Delta^{ (f) }_{j i} =\\Delta^{ (f-1) }_{j i} \\cdot  \\dfrac{\\partial s^{[N-(f+1)]}_{k}}{\\partial s^{[N-(f+2)]}_{i}} \\;,\n",
    "$$\n",
    "where the dot ($\\cdot$) indicates summation over repeated indices. For convinience we can also define $\\Delta^{(-1)}_{ji} = \\delta_{ij}$. \n",
    "\n",
    "\n",
    "For [#1](#1), $ f= N-2 $, i.e. $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial s^{(0)}_{p}} = \\Delta^{(N-2)}_{ji}$.\n",
    "\n",
    "For [#2](#2),  $f= N-(l+3)$, i.e. \n",
    "$$\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}} = \n",
    "\\Delta^{ (N-(l+3)) }_{r j} \\theta^{\\prime\\, (l+1)}_{j}s^{(l)}_{i} \\qquad\\qquad\\qquad\\qquad \n",
    "\\text{ for } l \\leq N -3\n",
    "\\\\\n",
    "\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(N-2)}_{ji}} = \n",
    "\\Delta^{ (-1) }_{r j} \\theta^{\\prime\\, (N-1)}_{j}s^{(N-2)}_{i} = \\delta_{r j} \\ \\theta^{\\prime\\, (N-1)}_{j}s^{(N-2)}_{i} \n",
    "\\qquad \\text{ for } l=N-2 \\; .\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly we can obtain the derivatives with respect to the biases, since\n",
    "$$\n",
    "\\dfrac{\\partial s^{(N-1)}_r}{\\partial b^{(l+1)}_{j}} = \\dfrac{\\partial s^{(N-1)}_{r}}{\\partial s^{(l+1)}_{j}} \\theta^{\\prime\\, (l+1)}_{j} \n",
    ";.\n",
    "$$\n",
    "\n",
    "This calculation can be done at the same time as  $\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}}$ since\n",
    "$$\n",
    "\\dfrac{\\partial s^{(N-1)}_{r}}{\\partial w^{(l)}_{ji}} = \\dfrac{\\partial s^{(N-1)}_r}{\\partial b^{(l+1)}_{j}} \\ s^{(l)}_{i} \\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
