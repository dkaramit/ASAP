{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a protopype for artificial neural netwok (I believe it is called a feed-forward ann, or something).\n",
    "\n",
    "In the data.txt, there are training data for the OR gate, but in principle you can use whatever you want.\n",
    "The limitaion of this particular implementation is that it only has one output. This is because I \n",
    "wrote it in just a few days (I had no idea about anns before starting writing), since I had to also \n",
    "finish my PhD. But I should be able to easily generalize it.\n",
    "\n",
    "The code is ugly, but works. It still needs some good convergence criteria.\n",
    "I hope to update it and make it better and clearer in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return nonlin(x)*(1-nonlin(x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def rnonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return nonlin(x,deriv)\n",
    "    return np.round(nonlin(x,deriv),0)\n",
    "\n",
    "def lin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return 1*np.power(x,0)\n",
    "    return np.power(x,1)\n",
    "\n",
    "def polyn(n,x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return n*np.power(x,float(n-1.))\n",
    "    return np.power(x,float(n))\n",
    "\n",
    "def relu(x,deriv=False):\n",
    "    relu_temp=[]\n",
    "    if deriv==False:\n",
    "        for i in x:\n",
    "            if i<=0:\n",
    "                relu_temp.append(0)\n",
    "            else:\n",
    "                relu_temp.append(i)\n",
    "        return relu_temp\n",
    "    if deriv:\n",
    "            if x<=0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "def relu_polyn(n,x,deriv=False):\n",
    "    relu_temp=[]\n",
    "    if deriv==False:\n",
    "        for i in x:\n",
    "            if i<=0:\n",
    "                relu_temp.append(0)\n",
    "            else:\n",
    "                relu_temp.append(polyn(n,i,deriv))\n",
    "        return relu_temp\n",
    "    if deriv:\n",
    "            if x<=0:\n",
    "                return 0\n",
    "            else:\n",
    "                return polyn(n,x,deriv)       \n",
    "\n",
    "\n",
    "    \n",
    "def sqerr(prediction,target,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return prediction-target\n",
    "    return 1/2.*np.power(prediction-target,2)\n",
    "\n",
    "\n",
    "\n",
    "#output_func is usually a linear function (because it can produce any real number), but play around and observe  \n",
    "def output_func(x,deriv=False):\n",
    "    return lin(x,deriv) #this gives 0 error!\n",
    "    #return np.round(nonlin(x,deriv),2) #it's good!\n",
    "    #return rnonlin(x,deriv)#this gives 0 error!\n",
    "    #return nonlin(x,deriv)\n",
    "\n",
    "def hidden_func(x,deriv=False): #define the function of the hidden layers\n",
    "    return nonlin(x,deriv)\n",
    "    #return lin(x,deriv)\n",
    "    #return polyn(1,x,deriv)\n",
    "\n",
    "def error_func(prediction,target,deriv=False):\n",
    "    return sqerr(prediction,target,deriv)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "def X_calculation(Signals,Inputs,Weights,Layers ):\n",
    "    for l in range(Layers-2): #range(L-2) because I'm going to fill the last xl and last signal outside the loop!\n",
    "        Signals[l]=np.array(Weights[l]).T.dot(np.array(Inputs[l]))\n",
    "        Inputs[l+1]=np.append(hidden_func(Signals[l]),1) #update xl and add the bias!\n",
    "    #the last signal: \n",
    "    Signals[Layers-2]=np.array(Weights[Layers-2]).T.dot(np.array(Inputs[Layers-2]))\n",
    "    Inputs[Layers-1]=output_func(Signals[Layers-2])\n",
    "\n",
    "\n",
    "def W_update(Inputs,Deltas,Layers,Layer_dim,L_Rate,W):\n",
    "    for l in range(0,Layers-1):    \n",
    "            for i in range(Layer_dim[l]+1):\n",
    "                for j in range(0,Layer_dim[l+1]):\n",
    "                    W[l][i][j]+=-L_Rate*Inputs[l][i]*Deltas[l][j] \n",
    "                    \n",
    "def BackProp(Signals,Weights,Error,Deltas,Layers,Layer_dim):\n",
    "    Deltas[L-2][0]=Error*output_func(Signals[Layers-2][0],True)\n",
    "    for l in reversed(range(Layers-2)):\n",
    "        for j in range(Layer_dim[l+1]):\n",
    "            Deltas[l][j]=0\n",
    "            for k in range(Layer_dim[l+2]):\n",
    "                Deltas[l][j]+=Weights[l+1][j][k]*Deltas[l+1][k]*hidden_func(Signals[l][j],True)\n",
    "                    # I use [Signals[l][j]] in hidden_func because I have assumed that the input is a list.\n",
    "                    #the [0] cancels the [...] in the input!\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0, 1, 1.0],\n",
       " [1.0, 0.0, 1, 1.0],\n",
       " [0.0, 1.0, 1, 1.0],\n",
       " [0.0, 0.0, 1, 0.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "X=[] #data to fit\n",
    "VX=[] #data to check\n",
    "temp1=[1]*5\n",
    "\n",
    "temp1.append(1)\n",
    "for line in open('data.txt'):\n",
    "    \n",
    "    temp=map(float,line.rstrip('\\n').split(','))\n",
    "    \n",
    "    if np.random.choice(temp1)==1:\n",
    "        X.append(temp)\n",
    "    else:\n",
    "        VX.append(temp)\n",
    "    \n",
    "#normalize each column to be in (0,1]\n",
    "max_col=np.max(X,axis=0)\n",
    "if len(VX)!=0:\n",
    "    max_colV=np.max(VX,axis=0)\n",
    "else:\n",
    "    max_colV=np.max(X,axis=0)\n",
    "#min_col=np.min(X,axis=0)\n",
    "#min_colV=np.min(VX,axis=0)\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[i])):\n",
    "        X[i][j]=(X[i][j])/max([max_col[j],max_colV[j]])\n",
    "    X[i].insert(len(X[i])-1,1)#add bias\n",
    "for i in range(len(VX)):\n",
    "    for j in range(len(VX[i])):\n",
    "        VX[i][j]=(VX[i][j])/max([max_col[j],max_colV[j]])\n",
    "    VX[i].insert(len(VX[i])-1,1)#add bias. I don't need it because I don't use it for training.\n",
    "    \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 8002\t\t|\t\t0.02490\t\t0.00167\n",
      "epochs: 16004\t\t|\t\t0.00008\t\t0.00000\n",
      "epochs: 24006\t\t|\t\t0.00000\t\t0.00000\n",
      "epochs: 32008\t\t|\t\t0.00000\t\t0.00000\n",
      "epochs: 40010\t\t|\t\t0.00000\t\t0.00000\n",
      "epochs: 48012\t\t|\t\t0.00000\t\t0.00000\n",
      "epochs: 56014\t\t|\t\t0.00000\t\t0.00000\n",
      "We have convergence\n",
      "model:\n",
      "[[[-3.225265275180998, -0.465514875024822], [-3.8519100096264927, -0.7747048498084319], [-0.462269072919738, -1.1396011314087777]], [[-2.9387439689785224], [0.8505728570209818], [0.9294973560514533]]]\n"
     ]
    }
   ],
   "source": [
    "datapoints=len(X)\n",
    "d=[len(X[0])-2,2,1] #dimension of each layer ( initial, [hidden] , 1 output)\n",
    "Learning_Rate=1\n",
    "Runs=100000\n",
    "L=len(d) #number of layers\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#initialize weights\n",
    "np.random.seed(1)\n",
    "w=[]\n",
    "for l in range(0,L-1):\n",
    "    temp1=[]\n",
    "    for i in range(d[l]+1):\n",
    "        temp1.append([])\n",
    "        for j in range(0,d[l+1]):\n",
    "            temp1[i].append(np.random.choice([np.random.random()*2-1]))\n",
    "    w.append(temp1)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#first, for simplicity, define the x's (empty array)\n",
    "xl=[]\n",
    "for l in range(L-1):\n",
    "    xl.append([0]*(d[l]+1)) #I've added d[l]+1 for the bias\n",
    "#define the last xl outside the loop, because it has no bias\n",
    "xl.append([0])    \n",
    "#add the bias for each l\n",
    "for l in range(L-1):\n",
    "    xl[l][len(xl[l])-1]=1    \n",
    "#I have to calculate the signals's, so in the same manner I can initialize the signals\n",
    "signal=[]\n",
    "for l in range(L-1):\n",
    "    signal.append([0]*(d[l+1])) #it's d[l+1] because there are as much signals in l as x's in l+1 (excluding the bias)\n",
    "#Since I have as many deltas as signals, I set delta=signal\n",
    "delta=signal[:]    \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "delta0=0\n",
    "conv=0\n",
    "Error=0\n",
    "st=0\n",
    "stt=0\n",
    "for epoch in range(1,Runs):\n",
    "    #calculate x[l] for one data point (and the signals!)\n",
    "    n=np.random.randint(len(X))\n",
    "    lenX=len(X[n])\n",
    "    xl[0]=X[n][:lenX-1]\n",
    "    xl[0]=np.array(xl[0])    \n",
    "    X_calculation(signal,xl,w,L )\n",
    "    BackProp(signal,w,error_func(xl[L-1][0],X[n][lenX-1:lenX][0],True),delta,L,d)\n",
    "    W_update(xl,delta,L,d,Learning_Rate,w)\n",
    "                \n",
    "#-----------------------------------------------------------------------------------------------------------                \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------                \n",
    "    \n",
    "    if st<datapoints*1000:  \n",
    "        st+=1\n",
    "        if X[n][lenX-1]!=0:\n",
    "            delta0+=np.abs((xl[L-1][0]-X[n][lenX-1])/float(X[n][lenX-1]))\n",
    "        if X[n][lenX-1]==0:\n",
    "            delta0+=np.abs(xl[L-1][0]-X[n][lenX-1])/10\n",
    "        if stt==2:\n",
    "            Error+=error_func(xl[L-1][0],X[n][lenX-1:lenX][0])\n",
    "    else:\n",
    "        stt+=1\n",
    "    if stt==1:\n",
    "        average1=delta0/float(st)\n",
    "        delta0=0\n",
    "        stt+=1\n",
    "        st=0\n",
    "    if stt==3:\n",
    "        average2=delta0/float(st)       \n",
    "        print'epochs: '+str(epoch)+'\\t\\t|\\t\\t'+ str(\"{:.5f}\".format(average1))+\"\\t\\t\"+str(\"{:.5f}\".format(average2))\n",
    "       \n",
    "        #------------calculate the delta's for the average error (it's slower but sometimes it converges quicker)! \n",
    "        #X_calculation(signal,xl,w,L )\n",
    "        #BackProp(signal,w,Error,delta,L,d)\n",
    "        #W_update(xl,delta,L,d,Learning_Rate,w)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------     \n",
    "        delta0=0\n",
    "        Error=0\n",
    "        st=0\n",
    "        stt=0            \n",
    "#------------------------------------------------------------------------------------------------            \n",
    "        #play around with the following if to change the point of exitting the loop\n",
    "\n",
    "        if average1==0 and average2<=0.05:\n",
    "            #print 'break epoch' '\\t' + str(epoch)\n",
    "            conv=1\n",
    "            break\n",
    "    \n",
    "        if np.abs(average1)<0.01 and np.abs(average1-average2)/np.abs(average1)<=0.05:\n",
    "            #print 'break epoch' '\\t' + str(epoch)\n",
    "            conv=1\n",
    "            break\n",
    "        if np.abs(average1)<0.30 and np.abs(average1-average2)/np.abs(average1)<0.1 and Learning_Rate>0.00001:\n",
    "            Learning_Rate=Learning_Rate/2\n",
    "            #print 'Learning Rate: \\t '+ str(Learning_Rate)\n",
    "        if Learning_Rate<=0.00001:\n",
    "            Learning_Rate=Learning_Rate*20\n",
    "            #print 'Learning Rate: \\t '+ str(Learning_Rate)\n",
    "if conv==1:\n",
    "    print 'We have convergence'\n",
    "else:\n",
    "    print 'We don\\'t really have convergence'\n",
    "print 'model:'\n",
    "print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFit\t\tTarget\n",
      "       1.0 \t        1.0\n",
      "       1.0 \t        1.0\n",
      "       1.0 \t        1.0\n",
      "       0.0 \t        0.0\n"
     ]
    }
   ],
   "source": [
    "test=1\n",
    "if test==1:\n",
    "    print '\\tFit' + '\\t\\t'+'Target'\n",
    "    for n in range(int(len(X))):\n",
    "        xl[0]=X[n][:len(X[n])-1]\n",
    "        xl[0]=np.array(xl[0])\n",
    "        X_calculation(signal,xl,w,L)    \n",
    "        print str(\"{:>10.2}\".format(xl[L-1][0]))+ ' \\t '+ str(\"{:>10.3}\".format(X[n][len(X[n])-1])) \n",
    "    for n in range(int(len(VX))):\n",
    "        xl[0]=VX[n][:len(VX[n])-1]\n",
    "        xl[0]=np.array(xl[0])\n",
    "        X_calculation(signal,xl,w,L)    \n",
    "        print str(\"{:>10.2}\".format(xl[L-1][0]))+ ' \\t '+ str(\"{:>10.3}\".format(VX[n][len(VX[n])-1])) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
