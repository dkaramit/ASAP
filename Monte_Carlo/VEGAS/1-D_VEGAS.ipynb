{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('nbAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the functions that I want to integrate\n",
    "def Func(x):\n",
    "    #return np.exp(-(x-0.3)**2/(2*0.1**2))\n",
    "    return np.sin(x*10)*np.exp(-(x-0.3)**2/(2*0.05**2))\n",
    "\n",
    "#the np.abs(Func) because it is needed for the calculation of the weights\n",
    "def absFunc(x):\n",
    "    return np.abs(Func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on what VEGAS tries to do.\n",
    "\n",
    "We have to calculate $I=\\int_{0}^{1}dx f(x)$ and we want to do it in Monte Carlo.\n",
    "We know that the variance is minimized if we sample from $p(x) \\sim |f(x)|$.\n",
    "\n",
    "What we do is to break the integration interval in N bins. In each bin we sample uniformly (the same number of sample in each bin). Now, if the bin-sizes are small in the regions that the integral has large contributions, then we effectivelly sample from $\\sim |f(x)|$ (that is the most important regions get more points!).\n",
    "\n",
    "VEGAS, iterativelly, tries to adjust each bin-size, so that the more important regions get smaller binsizes!\n",
    "See next comment on how the grid is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the numbers of bins and samples\n",
    "NBins=100#number of bins\n",
    "NSamplesPerBin=100 #number of samples per bin\n",
    "NSamples=NBins*NSamplesPerBin #total number of sampes\n",
    "\n",
    "#define the grid (initially uniform)\n",
    "GridPoints=np.linspace(0,1,NBins+1)#a grid of NBins bins needs NBins+1 points to be defined!\n",
    "\n",
    "#find the bin sizes (later I will defe a function to do this, beacuse I'll use it multiple times)\n",
    "BinSize=[]\n",
    "for i in range(NBins):\n",
    "    BinSize.append(GridPoints[i+1]-GridPoints[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the distribution (not really needed here)\n",
    "def Dist(i,number_of_bins=NBins):\n",
    "    '''\n",
    "    Since we have a constant distribution in each bin,\n",
    "    we only need the number of the bin as input.\n",
    "    '''\n",
    "    return 1/float(number_of_bins)*1/float(BinSize[i])\n",
    "\n",
    "#check the normalization\n",
    "#np.sum(map(lambda i: Dist(i)*BinSize[i],range(NBins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TotalInt(Func,Samples,GridPoints,BinSize):\n",
    "    global NBins,NSamplesPerBin\n",
    "    #calculate the integral of Func in [0,1]\n",
    "    Integral=0\n",
    "    for Bin in range(NBins):\n",
    "        _min=GridPoints[Bin]\n",
    "        _max=GridPoints[Bin+1]\n",
    "        for Sample in Samples[Bin]:\n",
    "            Integral+=Func(Sample)*BinSize[Bin]/float(NSamplesPerBin)\n",
    "\n",
    "    return Integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinInt(Func,Bin,Samples,GridPoints,BinSize):\n",
    "    global NBins,NSamplesPerBin\n",
    "    #calculate the integral of Func in the i Bin\n",
    "    Integral=0\n",
    "    _min=GridPoints[Bin]\n",
    "    _max=GridPoints[Bin+1]\n",
    "    for Sample in Samples[Bin]:\n",
    "        Integral+=Func(Sample)*BinSize[Bin]/float(NSamplesPerBin)\n",
    "\n",
    "    return Integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A commend on how the grid is updated:\n",
    "\n",
    "First, calculate $m_{i} \\equiv K \\times \\frac{\\int_{x_{i}}^{x_{i+1}} dx |f(x)|}{\\int_{0}^{1} dx |f(x)|}$ (K is a large constant) or for smoother convergence $m_{i} \\to K \\left[ \\times \\frac{\\frac{m_{i}}{K}-1}{log\\left( \\frac{m_{i}}{K} \\right) } \\right]^{\\alpha}$ ($\\alpha$ is a constand ~1)\n",
    "\n",
    "These m's weight how important the $i^{th}$ bin is. So we could subdevide each bin to $m_{i}+1$ sub-bins \n",
    "and then sample uniformly in each oe of them. However this would be extremely slow.\n",
    "So,we need to merge some of them to restore the original number of bins (N). To do this in a way \n",
    "that  we'll effectively approximate |f(x)|, we redefine the new $\\Delta x's$ as:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\frac{\\Delta x_{i}}{m_{i}+1}}{\\frac{\\Delta x_{i+1}}{m_{i+1}+1}}=\\frac{\\Delta x_{i}^{\\prime}}{\\Delta x_{i+1}^{\\prime}}\n",
    "$$\n",
    "\n",
    "In this way, we keep the relative density of the $m_{i}+1$ sub-divisions of each bin! \n",
    "The constraint in orde to do this consistently (we integrate in [0,1]) is:\n",
    "\n",
    "$$\n",
    "\\sum \\limits_{i=0}^{N-1} \\Delta x_{i}^{\\prime} =1\n",
    "$$\n",
    "\n",
    "The equations above give us\n",
    "$$\n",
    "\\Delta x_{i}^{\\prime}=\\frac{\\Delta x_{i}}{\\Delta x_{0}} \\frac{m_{0}+1}{m_{i}+1}\\Delta x_{0}^{\\prime}\n",
    "$$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\Delta x_{0}^{\\prime}=\\frac{\\Delta x_{0}}{m_{0}+1}\n",
    "\\left( \\sum \\limits_{i=0}^{N-1} \\frac{\\Delta x_{i}}{m_{i}+1} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "NOTE: When the algorithm converges, i.e $\\Delta x_{i}^{\\prime} \\approx \\Delta x_{i}$, \n",
    "then you can see that $ \\frac{m_{i}+1}{m_{0}+1} \\approx 1$. That is, VEGAS effectively\n",
    "breaks the interal of integration in such way that all bins contribute the same!\n",
    "\n",
    "---\n",
    "\n",
    "In some cases, one bin could so important that its contribution is very close to 100\\%.\n",
    "So we need to also smooth the m's (before doing anything else):\n",
    "\n",
    "$$\n",
    "m_{0} \\to \\frac{m_{0}+m_{1}}{2} \\\\\n",
    "m_{N-1} \\to \\frac{m_{N-1}+m_{N-2}}{2} \\\\\n",
    "m_{i} \\to \\frac{m_{i-1}+m_{i}+m_{i+1}}{3}\n",
    "$$\n",
    "\n",
    "NOTE: this makes the convergence slightly slower, but safer... So it should be optional and used if it fails otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93a6ef12ae8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#calculate the m_i's (call them weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mWeights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNBins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mTotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTotalInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsFunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGridPoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mBin\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNBins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mWeights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsFunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGridPoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBinSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mTotal\u001b[0m\u001b[0;31m#this is m_{i}/K in Lapage's paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5b03b21015a1>\u001b[0m in \u001b[0;36mTotalInt\u001b[0;34m(Func, Samples, GridPoints, BinSize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0m_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGridPoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBin\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mSample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mIntegral\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBinSize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNSamplesPerBin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NRuns=200 #number of runs. This refines the grid.\n",
    "alpha=0.1#the aplha parameter\n",
    "K=1000. #the K parameter\n",
    "_TINY=1.e-50#define a tiny number (if a weight is smaller than that, then set this weight to 1)\n",
    "_SMOOTH=False#Choose if you want to first smooth the m_{i}'s (see comment above)\n",
    "\n",
    "for Run in range(NRuns):\n",
    "    #Generate the samples\n",
    "    #initialize Samples array with dimensions [NSamplesPerBin][NBins] (for simplicity)\n",
    "    Samples=[[0 for i in range(NSamplesPerBin)] for j in range(NBins)]\n",
    "    for Bin in range(NBins):\n",
    "        _min=GridPoints[Bin]\n",
    "        for Sample in range(NSamplesPerBin):\n",
    "            #samples iin each bin (ie in [GridPoints[Bin],GridPoints[Bin+1]])\n",
    "            Samples[Bin][Sample]=np.random.rand()*BinSize[Bin]+_min\n",
    "\n",
    "\n",
    "    #calculate the m_i's (call them weights)\n",
    "    Weights=[ 0 for i in range(NBins)]\n",
    "    Total=TotalInt(absFunc,Samples,GridPoints,BinSize)\n",
    "    for Bin in range(NBins):\n",
    "        Weights[Bin]=BinInt(absFunc,Bin,Samples,GridPoints,BinSize)/Total#this is m_{i}/K in Lapage's paper.\n",
    "        #I need he log of Weights[Bin]. But if it is small, then the log is not a good choice\n",
    "      \n",
    "    if _SMOOTH:\n",
    "        #smooth the weights\n",
    "        Weights[0]=(Weights[0]+Weights[1])/2.\n",
    "        Weights[-1]=(Weights[-1]+Weights[-2])/2.\n",
    "        for Bin in range(1,NBins-1):\n",
    "            Weights[Bin]=(Weights[Bin-1]+Weights[Bin]+Weights[Bin+1])/3.\n",
    "\n",
    "    \n",
    "    for Bin in range(NBins):\n",
    "        if Weights[Bin]>_TINY:\n",
    "            Weights[Bin]=( (Weights[Bin]-1)/float(np.log(Weights[Bin])) )**alpha\n",
    "        else:\n",
    "            Weights[Bin]=Weights[Bin]**alpha#if Weights[Bin]<_TINY, then the log explodes. So it is better to se it this way\n",
    "            \n",
    "        Weights[Bin]=K*Weights[Bin]+1. #we should have at least one sub-bin for each bin!\n",
    "        \n",
    "\n",
    "    #initialize the new GridPoints\n",
    "    NewGridPoints=[0 for i in GridPoints]\n",
    "    NewBinSize=[0 for i in BinSize]\n",
    "\n",
    "\n",
    "    #calculate the newBinSize[0], so that all entries in newBinSize add up to 1. \n",
    "    _tmp=0\n",
    "    for j in range(NBins):\n",
    "        _tmp+=BinSize[j]/Weights[j]\n",
    "    NewBinSize[0]=_tmp**(-1)*BinSize[0]/Weights[0]\n",
    "\n",
    "    #You can now calculate the others as well\n",
    "    for i in range(1,NBins):\n",
    "        NewBinSize[i]=NewBinSize[0]*(BinSize[i]/BinSize[0]*Weights[0]/Weights[i])\n",
    "\n",
    "\n",
    "    #set the BinSize to be the NewBinSize\n",
    "    del BinSize\n",
    "    BinSize=NewBinSize[:]\n",
    "\n",
    "    #find the new GridPoints from the new BinSize\n",
    "    for i in range(1,NBins+1):\n",
    "        NewGridPoints[i]=NewGridPoints[i-1]+BinSize[i-1]\n",
    "\n",
    "    del GridPoints\n",
    "    GridPoints=NewGridPoints[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to interrupt at this point\n",
    "#raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to find appropriate stopping conditions for this.\n",
    "One idea is to run batches (of course!) and then see when the variance becomes acceptable!\n",
    "\n",
    "As it is stated in the literature, use the chi-squared estimator for the batches, because\n",
    "in this estimator the batch means with larger variance contribute less. Thus, the calculation \n",
    "is more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once the grid is refined, you can sample and see that the histogram of the samlpes, mathes the normalized |f(x)|.\n",
    "\n",
    "Samples=[[0 for i in range(NSamplesPerBin)] for j in range(NBins)]\n",
    "for Bin in range(NBins):\n",
    "    _min=GridPoints[Bin]\n",
    "    for Sample in range(NSamplesPerBin):\n",
    "        #samples iin each bin (ie in [GridPoints[Bin],GridPoints[Bin+1]])\n",
    "        Samples[Bin][Sample]=np.random.rand()*BinSize[Bin]+_min\n",
    "\n",
    "        \n",
    "plt.hist([i for f in Samples for i in f ],bins=NBins,density=1)\n",
    "\n",
    "from scipy.integrate import quad\n",
    "_norm=quad(absFunc,0,1)[0]\n",
    "_x=np.linspace(0,1,1000)\n",
    "_y=[absFunc(i)/_norm for i in _x]\n",
    "plt.plot(_x,_y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can see that the integral ca now be calculated\n",
    "Samples=[[0 for i in range(NSamplesPerBin)] for j in range(NBins)]\n",
    "\n",
    "for Bin in range(NBins):\n",
    "    _min=GridPoints[Bin]\n",
    "    for Sample in range(NSamplesPerBin):\n",
    "        #samples iin each bin (ie in [GridPoints[Bin],GridPoints[Bin+1]])\n",
    "        Samples[Bin][Sample]=np.random.rand()*BinSize[Bin]+_min\n",
    "\n",
    "\n",
    "TotalInt(Func,Samples,GridPoints,BinSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad(Func,0,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
