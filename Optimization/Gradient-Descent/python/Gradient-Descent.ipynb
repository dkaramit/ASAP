{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Shows Direction of Steepest Ascent\n",
    "\n",
    "The gradient of a function \"points\" towards the direction at which the function increases the most. \n",
    "\n",
    "To see how everything works, consider a function $f(\\vec{x})$ (with $\\vec{x}$ and $N$-dimensional vector) evaluated at some $\\vec{x} ^\\prime =\\vec{x} + \\delta \\vec{x}$ (with $\\delta \\vec{x}$ small). Then, we can write\n",
    "\n",
    "$$\n",
    "f(\\vec{x} + \\delta \\vec{x}) =  f(\\vec{x}) + \\delta \\vec{x} \\cdot \\vec\\nabla f(\\vec{x})  + \\dots \n",
    "$$\n",
    "with $\\vec\\nabla f(\\vec{x}) \\equiv \\dfrac{\\partial f(y_1,y_2,\\dots,N)}{\\partial y_i}\\Big|_{\\vec{y}=\\vec{x}}$.\n",
    "\n",
    "\n",
    "It becomes clear at this point that $f(\\vec{x})$ becomes maximally increased when $\\delta \\vec{x}$ is parallel to $ \\vec\\nabla f(\\vec{x})$, i.e. for $\\vec{x}^\\prime = \\vec{x} + \\alpha \\ \\vec\\nabla f(\\vec{x})$ (with some $\\alpha >0$). Also, $f(\\vec{x})$ is maximally decreased for  $\\vec{x}^\\prime = \\vec{x} - \\alpha \\ \\vec\\nabla f(\\vec{x})$ (with some $\\alpha >0$).\n",
    "\n",
    "One can easily understand this in the case $N=1,2,3$ as we can simply draw vectors and see what happens, but you can see <a href=\"#1\">footnote <sup>1</sup></a> for a general proof. \n",
    "\n",
    "\n",
    "### Gradient Acsent/Descent\n",
    "We would like to maximize or minimize $f(\\vec{x})$. To do this, we can exploit the previous observation by starting at some point $\\vec{x}$ and iteratively update $\\vec{x} \\to \\vec{x} \\pm \\alpha \\ \\vec\\nabla f(\\vec{x})$, until maximization ($+$) or minimization ($-$) occurs (say if $\\vec\\nabla f(\\vec{x})$ becomes very small).  \n",
    "\n",
    "The Gradient Acsent/Descent algorithm is maybe the simplest optimization algorith, as the only thing that needs is the derivative of the function. However, stopping conditions and local minima/maxima are a huge problem.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " <br>\n",
    " \n",
    " \n",
    "---\n",
    "<p id=\"#1\" style=\"font-size:8pt\" ><sup>1</sup>\n",
    "Basically, this statement comes from the fact that for two $R^N$ vectors, $\\vec{a},\\vec{b}$, it holds that $|\\vec{a}\\cdot \\vec{b}| \\leq ||\\vec{a}||||\\vec{b}||$. Here is the proof:\n",
    "<br>\n",
    "We start by defining the inner product as $$\\vec{a}\\cdot \\vec{b} \\equiv \\displaystyle\\sum_i a_i b_i.$$ Therefore, $$\\vec{a}\\cdot \\vec{a} = || \\vec a ||^2 \\geq 0.$$\n",
    "<br>\n",
    "With these definitions, we write\n",
    "$$\n",
    "\\Big|\\Big|\\vec{x} - \\dfrac{\\vec{x}\\cdot \\vec{y}}{||\\vec{y}||^2}\\vec{y} \\Big|\\Big|^2 \\geq 0 \n",
    "\\Rightarrow \n",
    "||\\vec{x}||^2 + \\dfrac{(\\vec{x}\\cdot \\vec{y})^2}{||\\vec{y}||^2} -2 \\dfrac{(\\vec{x}\\cdot \\vec{y})^2}{||\\vec{y}||^2} \\geq 0 \n",
    "\\Rightarrow \\\\\n",
    "||\\vec{x}||^2 - \\dfrac{(\\vec{x}\\cdot \\vec{y})^2}{||\\vec{y}||^2} \\geq 0 \n",
    "\\Rightarrow\n",
    "(\\vec{x}\\cdot \\vec{y})^2 \\leq ||\\vec{x}||^2 ||\\vec{y}||^2 \n",
    "\\Rightarrow \\\\\n",
    "|\\vec{x}\\cdot \\vec{y}| \\leq ||\\vec{x}|| ||\\vec{y}|| \\;.\n",
    "$$\n",
    "<br>\n",
    "Note also that this generalizes trivially to $C^N$ spaces to $\\langle y|x \\rangle \\leq ||\\vec{x}|| ||\\vec{y}||$, with  $\\langle y|x \\rangle \\displaystyle \\sum_i y^{*}_i x_i$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaDelta\n",
    "\n",
    "\n",
    "One of the things that can make GD unable to converge, is the learning rate, which has to be tunned. This can be avoided by using *AdaDelta* [[1212.5701](https://arxiv.org/abs/1212.5701)] that tries to adjust the learning as the algorithm proceeds. \n",
    "\n",
    "The update of $\\vec{x}$, according to this is: \n",
    "\n",
    "$$\n",
    "\\vec{x}^{(t)} = \\vec{x}^{(t-1)} - \\delta \\vec{x}^{(t)}  \\;,\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\delta \\vec{x}^{(t)} = \\dfrac{\\sqrt{ \\mathbb{E}[\\delta \\vec{x}^2]^{(t-1)} + \\epsilon}  }{\\sqrt{ \\mathbb{E}[g^2]^{(t)} + \\epsilon}} {\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)}) \\;, \n",
    "$$\n",
    "where the *decaying averages* are\n",
    "$$\n",
    "\\mathbb{E}[\\delta \\vec{x}^2]^{(t)} \\equiv \\gamma \\ \\mathbb{E}[\\delta \\vec{x}^2]^{(t-1)} +\n",
    "(1-\\gamma) \\ \\left( \\delta \\vec{x}^{(t)} \\right)^2 \\;, \n",
    "\\\\\n",
    "\\mathbb{E}[g^2]^{(t)} \\equiv \\gamma \\ \\mathbb{E}[g^2]^{(t-1)} +\n",
    "(1-\\gamma) \\ \\left( {\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)}) \\right) \\cdot \\left( {\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)}) \\right)\\;, \n",
    "$$\n",
    "for $\\mathbb{E}[X]^{(0)} = 0$. \n",
    "\n",
    "The parameters $\\gamma$ and $\\epsilon$ are free, but generally $\\gamma \\approx 1$ and $\\epsilon \\ll 1$. Common choice is $\\gamma \\approx 0.95$ and $\\epsilon \\ll 10^{-6}$.\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "With these definitions, the AdaDelta algorithm is:\n",
    "\n",
    "1. Initialize $\\mathbb{E}[g^2]^{(0)} =0$ and $\\mathbb{E}[\\delta \\vec{x}^2]^{(0)}=0$\n",
    "1. Loop t=1,2,...  until some condition is satisfied\n",
    "    1. Calculate ${\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)})$\n",
    "    1. Accumulate $\\mathbb{E}[g^2]^{(t)}$\n",
    "    1. Calculate $\\delta \\vec{x}^{t} = \\dfrac{\\sqrt{ \\mathbb{E}[\\delta \\vec{x}^2]^{(t-1)} + \n",
    "    \\epsilon}  }{\\sqrt{ \\mathbb{E}[g^2]^{(t)} + \\epsilon}} {\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)})$\n",
    "    1. Accumulate $\\mathbb{E}[\\delta \\vec{x}^2]^{(t)}$\n",
    "    1. Update $\\vec{x}^{(t+1)} = \\vec{x}^{(t)} - \\delta \\vec{x}^{(t)}$  \n",
    "1. Done\n",
    "\n",
    "---\n",
    "\n",
    "## RMSprop\n",
    "\n",
    "There is a variant of AdaDelta called *RMSprop*, which uses\n",
    "$$\n",
    "\\delta \\vec{x}^{(t)} = \\dfrac{\\alpha}{\\sqrt{ \\mathbb{E}[g^2]^{(t)} + \\epsilon}} {\\bf \\nabla}_\\vec{x} f(\\vec{x}^{(t-1)}) \\;, \n",
    "$$\n",
    "with $\\alpha \\ll 1$ some constant (typically $\\alpha  =10^{-3}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a Gradient Descent minimizer class to see how it works (maximization quite trivial once you get how it works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('nbAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the target should look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelFunc:\n",
    "    '''\n",
    "    This is how the function should look like.\n",
    "    The key point is to have way to update the parameters w.\n",
    "    '''\n",
    "    def __init__(self,func,dfdw_i,dimensions,w0):\n",
    "        '''\n",
    "        func: the function that depends on parameters w\n",
    "        dfdw_i: the derivative wrt w_i (see self.derivative_w to see how it should be defined).\n",
    "        dimensions:list  [inpunt_dim,output_dim]\n",
    "        w0: initial values of w\n",
    "        '''\n",
    "    \n",
    "        self.f=func\n",
    "        self.dfdw_i=dfdw_i\n",
    "        \n",
    "        self.w=w0\n",
    "        self.dim=len(w0)\n",
    "        \n",
    "        self.dimensions=dimensions\n",
    "        \n",
    "        self.signal=[0 for i in range(dimensions[1])] #will store the output of self.f\n",
    "        self.dsdw=[0 for i in range(dimensions[1])] #will store the output of self.dfdw_i\n",
    "        \n",
    "        #will store the input, to avoid passing the input every time we callthe function or its derivetive\n",
    "        self.input=[0 for i in range(dimensions[1])]\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.f(self)\n",
    "    \n",
    "    def setInput(self,x):\n",
    "        '''set the input'''\n",
    "        self.input=x\n",
    "    \n",
    "    def derivative_w(self,i):\n",
    "        self.dfdw_i(self,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lossFunc:\n",
    "    '''\n",
    "    This is how the loss function should look like.\n",
    "    We use a class, in order to encapsulate the gradient within the same object.\n",
    "    '''\n",
    "    def __init__(self,Q_i,dQds_i,model):\n",
    "        '''\n",
    "        Q: the loss function in one domension. It will be averaged over all dimensions in __call__\n",
    "        dQds_i: the derivative of the loss function wrt to s_i (the i^th signal)\n",
    "        \n",
    "        model: instance of modelFunc \n",
    "        '''\n",
    "        self.Q_i=Q_i\n",
    "        self.dQds_i=dQds_i\n",
    "        self.model=model\n",
    "        \n",
    "        \n",
    "        self.dim=len(self.model.w)#dimension of w\n",
    "        \n",
    "        self.N=model.dimensions[1]#dimension of model.signal\n",
    "            \n",
    "            \n",
    "        self.dQdw=0 #this will hold dQdw\n",
    "        \n",
    "        \n",
    "    def __call__(self,target):\n",
    "        sum_Q=0;\n",
    "        \n",
    "                \n",
    "        for r in range(self.N):\n",
    "            sum_Q+=self.Q_i(self.model,r,target[r])\n",
    "    \n",
    "        sum_Q=sum_Q/(float(self.N))\n",
    "\n",
    "        return sum_Q\n",
    "    \n",
    "    def grad(self,i,target):\n",
    "        self.model.derivative_w(i)\n",
    "        self.dQdw=0\n",
    "        \n",
    "        signal=self.model.signal\n",
    "        for dim in range(self.dim):\n",
    "            for r in range(self.N):\n",
    "                tmp_dQds=self.dQds_i(self.model,r,target[r])/(float(self.N))\n",
    "                self.dQdw += tmp_dQds*self.model.dsdw[r]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self,strategy):\n",
    "            self.strategy=strategy\n",
    "    \n",
    "    def run(self,abs_tol=1e-5, rel_tol=1e-3, step_break=100,max_step=5000):\n",
    "        '''        \n",
    "        abs_tol, rel_tol, step_break: stop when _check<1 (_check is what update should return) \n",
    "        for step_break consecutive steps\n",
    "        \n",
    "        max_step: maximum number of steps\n",
    "        '''\n",
    "        _s=0\n",
    "        count_steps=1\n",
    "        while count_steps<=max_step:\n",
    "            _check=self.strategy.update(abs_tol, rel_tol)\n",
    "            \n",
    "            count_steps+=1             \n",
    "                \n",
    "            \n",
    "            if _check<1:\n",
    "                _s+=1\n",
    "            else:\n",
    "                _s=0\n",
    "            \n",
    "            if _s>step_break:\n",
    "                break\n",
    "\n",
    "        return self.strategy.Q.model.w[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGD:\n",
    "    def __init__(self,loss,data_in,data_out,alpha=1e-2):\n",
    "        '''\n",
    "        loss: the loss function\n",
    "        data_in, data_out: the input, output data to be used in order to minimize the loss\n",
    "        alpha: the learning rate\n",
    "        '''\n",
    "        \n",
    "        self.Q=loss\n",
    "        self.data_in=data_in\n",
    "        self.data_out=data_out\n",
    "        self.alpha=alpha\n",
    "\n",
    "        self.data_size=len(self.data_in)\n",
    "        self.steps=[]\n",
    "        self.steps.append(self.Q.model.w[:])\n",
    "        self.dim=self.Q.model.dim\n",
    "        \n",
    "        #we need this to hold the average gadient over all data points\n",
    "        self.grad=[0 for _ in range(self.dim)]\n",
    "\n",
    "\n",
    "    def update(self,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        update should return a number that when it is smaller than 1\n",
    "        the main loop stops. Here I choose this number to be:\n",
    "        sqrt(1/dim*sum_{i=0}^{dim}(grad/(abs_tol+x*rel_tol))_i^2)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        _w2=0\n",
    "        _check=0\n",
    "        \n",
    "\n",
    "        #get the average gradient over all data\n",
    "        for index in range(self.data_size):\n",
    "            t=self.data_out[index]\n",
    "    \n",
    "            self.Q.model.setInput(self.data_in[index])\n",
    "            self.Q.model()\n",
    "\n",
    "            for i in range(self.dim):\n",
    "                self.Q.grad(i,t)\n",
    "                self.grad[i]+=self.Q.dQdw/self.data_size\n",
    "                \n",
    "               \n",
    "        for i in range(self.dim):\n",
    "            dw=self.alpha*self.grad[i]\n",
    "            self.Q.model.w[i]=self.Q.model.w[i]-dw\n",
    "\n",
    "            _w2=abs_tol + self.Q.model.w[i] * rel_tol\n",
    "            _check+=(dw/_w2)*(dw/_w2)\n",
    "            \n",
    "            self.grad[i]=0\n",
    "\n",
    "        _check=np.sqrt(1./self.dim *_check)\n",
    "\n",
    "        self.steps.append(self.Q.model.w[:])\n",
    "        \n",
    "        \n",
    "        return _check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSpropGD:\n",
    "    '''Implementation of RMSprop.'''\n",
    "\n",
    "    def __init__(self,loss,data_in,data_out,gamma=0.95,epsilon=1e-6,alpha=1e-3):\n",
    "        '''\n",
    "        loss: the loss function\n",
    "        data_in, data_out: the input, output data to be used in order to minimize the loss\n",
    "        gamma: the decaying parameter\n",
    "        epsilon: safety parameter (to avoid division by 0)\n",
    "        alpha: learning rate\n",
    "        '''\n",
    "        \n",
    "        self.Q=loss\n",
    "        self.data_in=data_in\n",
    "        self.data_out=data_out\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.data_size=len(self.data_in)\n",
    "        self.steps=[]\n",
    "        self.steps.append(self.Q.model.w[:])\n",
    "        self.dim=self.Q.model.dim\n",
    "        \n",
    "        # counters for the decaying means of the gradient and dw         \n",
    "        self.gE=[0 for _ in self.Q.model.w]\n",
    "        #we need this to hold the average gadient over all data points\n",
    "        self.grad=[0 for _ in range(self.dim)]\n",
    "\n",
    "    def update(self,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        update should return a number that when it is smaller than 1\n",
    "        the main loop stops. Here I choose this number to be:\n",
    "        sqrt(1/dim*sum_{i=0}^{dim}(grad/(abs_tol+x*rel_tol))_i^2)\n",
    "        '''\n",
    "        _w2=0\n",
    "        _check=0\n",
    "        \n",
    "        #get the average gradient over all data\n",
    "        for index in range(self.data_size):\n",
    "            t=self.data_out[index]\n",
    "    \n",
    "            self.Q.model.setInput(self.data_in[index])\n",
    "            self.Q.model()\n",
    "\n",
    "            for i in range(self.dim):\n",
    "                self.Q.grad(i,t)\n",
    "                self.grad[i]+=self.Q.dQdw/self.data_size\n",
    "\n",
    "        for i in range(self.dim):\n",
    "            self.gE[i]=self.gamma*self.gE[i] + (1-self.gamma)*self.grad[i]**2 \n",
    "            dw=self.alpha/np.sqrt( (self.gE[i]+self.epsilon)  )*self.grad[i]\n",
    "            \n",
    "            self.Q.model.w[i]=self.Q.model.w[i]-dw\n",
    "\n",
    "            _w2=abs_tol + self.Q.model.w[i] * rel_tol\n",
    "            _check+=(dw/_w2)*(dw/_w2)\n",
    "            self.grad[i]=0\n",
    "\n",
    "        _check=np.sqrt(1./self.dim *_check)\n",
    "\n",
    "        self.steps.append(self.Q.model.w[:])\n",
    " \n",
    "        return _check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDeltaGD:\n",
    "    '''Implementation of AdaDelta.'''\n",
    "    \n",
    "    def __init__(self,loss,data_in,data_out,gamma=0.95,epsilon=1e-6,alpha=1):\n",
    "        '''\n",
    "        loss: the loss function\n",
    "        data_in, data_out: the input, output data to be used in order to minimize the loss\n",
    "        gamma: the decaying parameter\n",
    "        epsilon: safety parameter (to avoid division by 0)\n",
    "        '''\n",
    "        self.Q=loss\n",
    "        self.data_in=data_in\n",
    "        self.data_out=data_out\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.data_size=len(self.data_in)\n",
    "        self.steps=[]\n",
    "        self.steps.append(self.Q.model.w[:])\n",
    "        self.dim=self.Q.model.dim\n",
    "        \n",
    "        # counters for the decaying means of the gradient and dw         \n",
    "        self.gE=[0 for _ in self.Q.model.w]\n",
    "        self.dwE=[0 for _ in self.Q.model.w]\n",
    "        #we need this to hold the average gadient over all data points\n",
    "        self.grad=[0 for _ in range(self.dim)]\n",
    "\n",
    "    def update(self,abs_tol=1e-5, rel_tol=1e-3):\n",
    "        '''\n",
    "        update should return a number that when it is smaller than 1\n",
    "        the main loop stops. Here I choose this number to be:\n",
    "        sqrt(1/dim*sum_{i=0}^{dim}(grad/(abs_tol+x*rel_tol))_i^2)\n",
    "        '''\n",
    "        _w2=0\n",
    "        _check=0\n",
    "        \n",
    "        #get the average gradient over all data\n",
    "        for index in range(self.data_size):\n",
    "            t=self.data_out[index]\n",
    "    \n",
    "            self.Q.model.setInput(self.data_in[index])\n",
    "            self.Q.model()\n",
    "\n",
    "            for i in range(self.dim):\n",
    "                self.Q.grad(i,t)\n",
    "                self.grad[i]+=self.Q.dQdw/self.data_size\n",
    "                \n",
    "        for i in range(self.dim):\n",
    "            \n",
    "            self.gE[i]=self.gamma*self.gE[i] + (1-self.gamma)*self.grad[i]**2 \n",
    "            dw=np.sqrt( (self.dwE[i]+self.epsilon)/(self.gE[i]+self.epsilon)  )*self.grad[i]*self.alpha\n",
    "            \n",
    "            self.dwE[i]=self.gamma*self.dwE[i] + (1-self.gamma)*dw**2\n",
    "            \n",
    "            self.Q.model.w[i]=self.Q.model.w[i] - dw\n",
    "            \n",
    "            \n",
    "            _w2=abs_tol + self.Q.model.w[i] * rel_tol\n",
    "            _check+=(dw/_w2)*(dw/_w2)\n",
    "            self.grad[i]=0\n",
    "\n",
    "        _check=np.sqrt(1./self.dim *_check)\n",
    "        \n",
    "        self.steps.append(self.Q.model.w[:])\n",
    "        \n",
    "        return _check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Lets find $w_1$ and $w_2$ for $f(x) = x w_1+ w_2$ that fit with data taken from $t=2*x+3$.\n",
    "\n",
    "Obviously the answer is $w_1 = 2$ and $w_2 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the squared error and its derivative\n",
    "def Q_i(model, i, target):\n",
    "    return (model.signal[i]-target)**2\n",
    "\n",
    "\n",
    "def dQds_i(model, i, target):\n",
    "    return 2*(model.signal[i]-target)\n",
    "\n",
    "\n",
    "#the model and its derivative wrt w\n",
    "def f(self):\n",
    "    self.signal[0]=self.input[0]*self.w[0]+ self.w[1]\n",
    "\n",
    "\n",
    "\n",
    "def dfdw_i(self, i):\n",
    "    if i==0:\n",
    "        self.dsdw[0]=self.input[0]\n",
    "    if i==1:\n",
    "        self.dsdw[0]=1\n",
    "        \n",
    "        \n",
    "#declare the instances\n",
    "model=modelFunc(f,dfdw_i,[1,1],[1,0.2])\n",
    "Q=lossFunc(Q_i,dQds_i,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in=[]\n",
    "data_out=[]\n",
    "xmin=-5\n",
    "xmax=2\n",
    "for i in range(100):\n",
    "    x=np.random.rand()*(xmax-xmin)+xmin\n",
    "    \n",
    "    data_in.append( [x])\n",
    "    data_out.append( [2*x+3])\n",
    "\n",
    "# strategy=VanillaGD(Q ,data_in,data_out,alpha=1e-2)\n",
    "# strategy=RMSpropGD(Q ,data_in,data_out,gamma=0.995,epsilon=1e-6,alpha=1e-2)\n",
    "strategy=AdaDeltaGD(Q ,data_in,data_out,gamma=0.99,epsilon=1e-5,alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD=GradientDescent(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.9999533636454934, 2.999827046401646], 558)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD.run(abs_tol=1e-4, rel_tol=1e-4, step_break=250,max_step=15000),len(strategy.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
